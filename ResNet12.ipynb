{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using the implemented Blocks to build a simple ResNet ","metadata":{}},{"cell_type":"markdown","source":"<img src=\"data/The structure of ResNet 12.png\" hight=\"600\" width=\"500\">","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn .functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n# from utils.utils import conv1x1, conv3x3\n# from blocks import BasicBlock, BottleneckBlock","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:43:38.974542Z","iopub.execute_input":"2022-06-06T19:43:38.974863Z","iopub.status.idle":"2022-06-06T19:43:38.979146Z","shell.execute_reply.started":"2022-06-06T19:43:38.974835Z","shell.execute_reply":"2022-06-06T19:43:38.978646Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    \n    def __init__(\n        self, \n        channel_size,\n        shortcut_method: str = \"identity\",\n        stride: int = 1\n    )-> None:\n        \n        super(BasicBlock, self).__init__()\n        # first layer\n        self.conv1 = conv3x3(channel_size, channel_size, stride=stride, padding=1)\n        # BatchNorm is an element-wise operation and therefore, it does not change the size of our volume.\n        self.BN1 = nn.BatchNorm2d(channel_size) \n        \n        # second layer\n        self.conv2 = conv3x3(channel_size, channel_size, padding=1)\n        # BatchNorm is an element-wise operation and therefore, it does not change the size of our volume.\n        self.BN2 = nn.BatchNorm2d(channel_size)\n        \n        # for projection short-cut (same input(x) channels, and downsampling it to channel_size)\n        self.shortcut_method = shortcut_method \n        self.shortcut = conv1x1(channel_size, channel_size, stride=stride)\n        # activation function\n        self.relu = nn.ReLU(inplace=True)\n    \n    def forward(\n        self, \n        x:torch.Tensor\n    ) -> torch.Tensor:\n        # shortcut path may be identity short-cut OR projection short-cut\n        # identity short-cut\n        if self.shortcut_method.lower()=='identity':\n            shortcut = x\n        elif self.shortcut_method.lower()=='projection':\n            # projection short-cut\n            shortcut = self.shortcut(x)\n        else:\n            raise ValueError(\"ResidualBlock only supports identity or projection shortcut\")\n            \n        ################\n        # Forward Path #\n        ################\n        # first layer\n        out = self.conv1(x)\n        out = self.BN1(out)\n        out = self.relu(out)\n        # second layer\n        out = self.conv2(out)\n        out = self.BN2(out)\n        \n        ##########\n        # Output #\n        ##########\n        #out (layers output + shortcut output)\n        out += shortcut\n        out = self.relu(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:15.030706Z","iopub.execute_input":"2022-06-06T19:46:15.031158Z","iopub.status.idle":"2022-06-06T19:46:15.045109Z","shell.execute_reply.started":"2022-06-06T19:46:15.031097Z","shell.execute_reply":"2022-06-06T19:46:15.043969Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def conv3x3(in_channels: int,\n            out_channels: int,\n            padding: int = 1,\n            stride: int = 1):\n    \"\"\"\n    3x3 convolution with padding for Basic Block\n    \"\"\"\n    return nn.Conv2d(\n        in_channels,\n        out_channels,\n        kernel_size=3, # 3x3\n        stride=stride,\n        padding=padding,\n        bias=False,\n    )\n\n\ndef conv1x1(in_channels: int,\n            out_channels: int,\n            padding: int = 1,\n            stride: int = 1):\n    \"\"\"\n    1x1 convolution (channel-wise pooling) for Bottleneck Block \n    structure and the conv-connection in both Bottleneck and \n    Basic blocks residual connetions.\n    out = ((i+2p-K)/S)+1, here it will have the same dimensions\n    of input but down sample the depth or number of feature maps.\n    It's like a linear weighting or projection of the input.\n    \"\"\"\n    return nn.Conv2d(\n        in_channels,\n        out_channels, # number of kernels\n        padding=padding,\n        kernel_size=1, # 1x1\n        stride=stride, \n        bias=False\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:15.559819Z","iopub.execute_input":"2022-06-06T19:46:15.560386Z","iopub.status.idle":"2022-06-06T19:46:15.568685Z","shell.execute_reply.started":"2022-06-06T19:46:15.560347Z","shell.execute_reply":"2022-06-06T19:46:15.567839Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nnum_epochs = 20\nbatch_size = 100\nlearning_rate = 0.001","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:15.824900Z","iopub.execute_input":"2022-06-06T19:46:15.825371Z","iopub.status.idle":"2022-06-06T19:46:15.830228Z","shell.execute_reply.started":"2022-06-06T19:46:15.825332Z","shell.execute_reply":"2022-06-06T19:46:15.829436Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### Load and prepare the data","metadata":{}},{"cell_type":"code","source":"# Image preprocessing modules\ntransform = transforms.Compose([\n    transforms.Pad(4),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32),\n    transforms.ToTensor()])\n\n# CIFAR-10 dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n                                             train=True, \n                                             transform=transform,\n                                             download=True)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n                                            train=False, \n                                            transform=transforms.ToTensor())","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:16.438883Z","iopub.execute_input":"2022-06-06T19:46:16.439380Z","iopub.status.idle":"2022-06-06T19:46:17.458261Z","shell.execute_reply.started":"2022-06-06T19:46:16.439340Z","shell.execute_reply":"2022-06-06T19:46:17.457330Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [45000, 5000])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:17.459879Z","iopub.execute_input":"2022-06-06T19:46:17.460319Z","iopub.status.idle":"2022-06-06T19:46:17.469568Z","shell.execute_reply.started":"2022-06-06T19:46:17.460277Z","shell.execute_reply":"2022-06-06T19:46:17.468716Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                         batch_size=batch_size,\n                                         shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:17.482920Z","iopub.execute_input":"2022-06-06T19:46:17.483330Z","iopub.status.idle":"2022-06-06T19:46:17.492318Z","shell.execute_reply.started":"2022-06-06T19:46:17.483295Z","shell.execute_reply":"2022-06-06T19:46:17.491258Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def imshow(inp, title=None):\n    \"\"\"imshow for Tensor.\"\"\"\n    plt.figure(figsize=(20, 8))\n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = np.clip(inp, 0, 1)\n#     plt.title(str(' '*18).join(title.numpy()).title())\n    plt.imshow(inp)\n\n\n# Get a batch of training data\nimages, labels = next(iter(test_loader))\n\n# Make a grid from batch\noutput = torchvision.utils.make_grid(images[:16])\n\nimshow(output, title=labels[:16])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T20:41:37.833807Z","iopub.execute_input":"2022-06-06T20:41:37.834245Z","iopub.status.idle":"2022-06-06T20:41:38.055659Z","shell.execute_reply.started":"2022-06-06T20:41:37.834208Z","shell.execute_reply":"2022-06-06T20:41:38.054787Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### ResNet using the Basic Block","metadata":{}},{"cell_type":"code","source":"# ResNet\nclass ResNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = conv3x3(3, 64)\n        self.maxpool = nn.MaxPool2d(3)\n#         self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.block1 = BasicBlock.BasicBlock(64)\n        self.block2 = BasicBlock.BasicBlock(64)\n        self.block3 = BasicBlock.BasicBlock(64)\n        self.block4 = BasicBlock.BasicBlock(64)\n        self.block5 = BasicBlock.BasicBlock(64)\n        self.avg_pool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64, num_classes)\n        \n        \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.maxpool(out)\n        out = self.relu(out)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:23.665254Z","iopub.execute_input":"2022-06-06T19:46:23.665809Z","iopub.status.idle":"2022-06-06T19:46:23.679753Z","shell.execute_reply.started":"2022-06-06T19:46:23.665774Z","shell.execute_reply":"2022-06-06T19:46:23.678925Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#########################################\n#            Trainer Class              #\n#########################################\n\nclass Trainer:\n    \n    def __init__(self, model, optimizer, criterion, scheduler=None, load_path=None):\n        self.__class__.__name__ = \"PyTorch Trainer\"\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.scheduler = scheduler\n        tqdm.refresh\n        ## Setup Metric class\n        self.metrics = namedtuple('Metric', ['loss', 'train_error', 'val_error'])\n        \n        # if model exist\n        if load_path:\n            self.model = torch.load(load_path)\n        \n    def save_model(self, path):\n        torch.save(self.model.state_dict(), path)\n        \n    def run(self, train_loader, val_loader):\n        min_valid_loss = np.inf\n        ## Setup Metric class\n        Metric = namedtuple('Metric', ['loss', 'train_error', 'val_error'])\n        self.metrics = []\n        self.model.train() \n        for epoch in range(num_epochs):\n            epoch_loss = 0.0\n            correct = 0\n            total = 0\n            data_iter = enumerate(train_loader)\n            t_prog_bar = tqdm(range(len(train_loader)))\n#             lr = self.scheduler.get_last_lr()[0]\n            lr = self.optimizer.param_groups[0]['lr']\n            for step in t_prog_bar: # iter over batches\n                \n                # get the input images and their corresponding labels\n                batch_idx, (data, labels) = next(data_iter)\n                # clear the gradient\n                self.optimizer.zero_grad()\n                # wrap them in a torch Variable and move tnsors to the configured device\n                images, labels = Variable(data).to(device), Variable(labels).to(device)                                  \n                # Forward Pass\n                target = self.model(data)\n                # get the predicted class from the maximum value in the output-list of class scores\n                pred = target.data.max(1, keepdim=True)[1]\n                predicted = pred.eq(labels.data.view_as(pred))\n                # Count the correct\n                correct += predicted.sum()\n                # Find the Loss\n                loss = self.criterion(target, labels)\n#                 loss = F.nll_loss(target, labels)\n                # Backward Pass\n                # Calculate gradients\n                loss.backward()\n                # Update Weights\n                self.optimizer.step()\n                # Calculate total Loss\n                epoch_loss += loss.item()\n                # Calculate total samples\n                total += labels.size(0)\n                \n                t_prog_bar.set_description('Epoch {}/{}, Loss: {:.4f}, lr={:.7f}'.format(epoch+1,num_epochs,loss.item(),lr))\n                \n#                 torch.cuda.empty_cache()\n                del images\n                del labels\n                del loss\n                \n            valid_loss = 0.0\n            self.model.eval() # Optional when not using Model Specific layer\n            with torch.no_grad():\n                for data, labels in val_loader:\n                    # Forward Pass\n                    target = self.model(data)\n                    # Find the Loss\n                    loss = self.criterion(target,labels)\n    #                 loss = F.nll_loss(target,labels)\n                    # Calculate Loss\n                    valid_loss += loss.item()\n                    t_prog_bar.set_description('Epoch {}/{}, Loss: {:.4f}, Val_Loss: {:.4f}, lr={:.7f}'\\\n                                               .format(epoch+1, num_epochs, loss.item(), valid_loss, lr))\n\n            #Check point\n            if min_valid_loss > valid_loss:\n                print('Validation Loss Decreased ({:.6f} ===> {:.6f}) \\nSaving The Model'.format(min_valid_loss/len(val_loader), \n                                                                                                 valid_loss/len(val_loader)))\n\n                min_valid_loss = valid_loss/len(val_loader)\n\n            total_loss = epoch_loss/len(train_loader.dataset)\n            train_error = 1.0 - correct/len(train_loader.dataset)  # 1 - acc\n            val_error = 1.0 - inference(self.model, val_loader)  # 1 - acc\n\n            self.metrics.append(Metric(loss=epoch_loss, \n                                       train_error=train_error,\n                                       val_error=val_error))\n            \n            # Decrease the lr\n            scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:24.095556Z","iopub.execute_input":"2022-06-06T19:46:24.095919Z","iopub.status.idle":"2022-06-06T19:46:24.118807Z","shell.execute_reply.started":"2022-06-06T19:46:24.095891Z","shell.execute_reply":"2022-06-06T19:46:24.117960Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#########################################\n#             Define the model          #\n#           Define the optimizer        #\n#           Define the criterion        #\n#           Show model summary          #\n#########################################\ntorch_model = ResNet()\n# define the criterion\ncriterion = nn.CrossEntropyLoss()\n# define the optimizer\noptimizer = torch.optim.Adam(torch_model.parameters(), lr=learning_rate)\n# define the scheduler (learninig rate decaying)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.05)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:24.673674Z","iopub.execute_input":"2022-06-06T19:46:24.674083Z","iopub.status.idle":"2022-06-06T19:46:24.690434Z","shell.execute_reply.started":"2022-06-06T19:46:24.674045Z","shell.execute_reply":"2022-06-06T19:46:24.689079Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#########################################\n#             Model inference           #\n#########################################\ndef inference(model, loader):\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for data, labels in loader:\n            images = Variable(data).to(device)\n            labels = Variable(labels).to(device)\n            output = model(images)\n            pred = output.data.max(1, keepdim=True)[1]\n            predictions = pred.eq(labels.data.view_as(pred))\n            total += labels.size(0)\n            correct += (predictions == labels).sum().item()\n        return correct / total","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:25.610226Z","iopub.execute_input":"2022-06-06T19:46:25.611533Z","iopub.status.idle":"2022-06-06T19:46:25.621727Z","shell.execute_reply.started":"2022-06-06T19:46:25.611489Z","shell.execute_reply":"2022-06-06T19:46:25.621006Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#########################################\n#      Start Training the model         #\n#########################################\ntrainer = Trainer(torch_model, optimizer, criterion)\ntrainer.run(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T19:46:26.511345Z","iopub.execute_input":"2022-06-06T19:46:26.512093Z","iopub.status.idle":"2022-06-06T20:28:48.355002Z","shell.execute_reply.started":"2022-06-06T19:46:26.512049Z","shell.execute_reply":"2022-06-06T20:28:48.354087Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Show the results","metadata":{}},{"cell_type":"code","source":"test = iter(test_loader)\nplt.figure(figsize=(20, 8))\nfor i in range(16):\n    image, label = next(test)\n    idx = np.random.randint(batch_size, size=1)[0]\n    output = torch_model(image[[idx]])\n    predicted = output.data.max(1, keepdim=True)[1]\n    pred = predicted[0][0]\n    act = label[idx].numpy().tolist()\n    plt.tight_layout()\n    plt.subplot(4,4,i+1)\n    plt.title(\"Predicted: {} \\nTrue: {}\".format(pred, act),color=(\"green\" if pred==act else \"red\"))\n    plt.imshow((np.clip(image[idx].numpy().transpose((1, 2, 0)), 0, 1)))\n    plt.xticks([])\n    plt.yticks([])","metadata":{"execution":{"iopub.status.busy":"2022-06-06T20:45:16.748025Z","iopub.execute_input":"2022-06-06T20:45:16.748400Z","iopub.status.idle":"2022-06-06T20:45:18.134247Z","shell.execute_reply.started":"2022-06-06T20:45:16.748369Z","shell.execute_reply":"2022-06-06T20:45:18.133407Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"### It's acceptable with this simple model and number of epochs","metadata":{}}]}